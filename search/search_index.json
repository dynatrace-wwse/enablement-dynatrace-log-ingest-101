{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"1. About","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"#lab-overview","title":"Lab Overview","text":"<p>During this hands-on training lab, we\u2019ll learn how to capture logs from Kubernetes using the Dynatrace Operator to deploy the Dynatrace Log Module.  We'll then configure log monitoring in Dynatrace to maximize the value that we get from logs.  Finally, we'll analyze the logs in context using the various apps native to the Dynatrace platform.</p> <p>Lab tasks:</p> <ol> <li> <p>Launch GitHub codespaces container with lab setup</p> <ul> <li>Kubernetes cluster running AstroShop demo application</li> </ul> </li> <li> <p>Deploy Kubernetes Platform Monitoring + Application Observability</p> </li> <li> <p>Configure and validate Kubernetes log ingest into Dynatrace</p> </li> <li> <p>Ingest CronJob logs</p> </li> <li> <p>Configure advanced log monitoring in Dynatrace</p> <ul> <li>Log module feature flags</li> <li>Sensitive data masking</li> <li>Timestamp patterns and splitting</li> <li>Dynatrace component logs for self-monitoring</li> </ul> </li> <li> <p>Configure Dynatrace OpenPipeline for log transformation on ingest</p> </li> <li> <p>Analyze logs in context using apps native to the Dynatrace platform</p> <ul> <li>Problems</li> <li>Kubernetes</li> <li>Distributed Tracing</li> <li>Services</li> </ul> </li> <li> <p>Clean up GitHub codespaces instance</p> </li> </ol>"},{"location":"#technical-specification","title":"Technical Specification","text":""},{"location":"#technologies-used","title":"Technologies Used","text":"<ul> <li>Dynatrace</li> <li>Kubernetes Kind<ul> <li>tested on Kind tag 0.27.0</li> </ul> </li> <li>Cert Manager - *prerequisite for OpenTelemetry Operator<ul> <li>tested on cert-manager v1.15.3</li> </ul> </li> <li>Dynatrace Operator<ul> <li>tested on v1.4.2 (April 2025)</li> </ul> </li> <li>Dynatrace OneAgent<ul> <li>tested on v1.309 (April 2025)</li> </ul> </li> </ul>"},{"location":"#reference-architecture","title":"Reference Architecture","text":"<p>Dynatrace on Kubernetes: Application observability</p> <p>Dynatrace on Kubernetes: Platform monitoring</p> <p>OpenTelemetry Astronomy Shop Demo Architecture</p>"},{"location":"#continue","title":"Continue","text":"<p>In the next section, we'll review the prerequisites for this lab needed before launching our Codespaces instance.</p> <ul> <li>Continue to getting started</li> </ul>"},{"location":"2-getting-started/","title":"2. Getting started","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"2-getting-started/#prerequisites","title":"Prerequisites","text":"<p>You will need full administrator access to a Dynatrace SaaS tenant with a DPS license.</p> <ul> <li>Enable OpenTelemetry OneAgent Features</li> </ul>"},{"location":"2-getting-started/#enable-opentelemetry-oneagent-features","title":"Enable OpenTelemetry OneAgent Features","text":"<p>The demo application in this lab, AstroShop, contains OpenTelemetry instrumentation that can be picked up by OneAgent.</p> <p>Navigate to the <code>Settings Classic</code> app in the Dynatrace tenant.  Open <code>OneAgent Features</code> from the Preferences sub-menu.  Search for features that contain the word <code>OpenTelemetry</code>.  Enable all OneAgent features for OpenTelemetry.</p> <p></p>"},{"location":"2-getting-started/#continue","title":"Continue","text":"<p>In the next section, we'll launch our Codespaces instance.</p> <ul> <li>Continue to Codespaces</li> </ul>"},{"location":"3-codespaces/","title":"3. Codespaces","text":""},{"location":"3-codespaces/#create-codespace","title":"Create Codespace","text":"<p>Click to open Codespaces for this lab repository:</p> <p></p> <p>Codespace Configuration</p> <ul> <li>Branch<ul> <li>select the main branch</li> </ul> </li> <li>Dev container configuration<ul> <li>select Enablement on codespaces template</li> </ul> </li> <li>Machine type<ul> <li>select 4-core</li> </ul> </li> <li>Region<ul> <li>select any region, preferably one closest to your Dynatrace tenant</li> </ul> </li> </ul>"},{"location":"3-codespaces/#wait-for-codespace","title":"Wait for Codespace","text":"<p>We know your time is very valuable. This codespace takes around 7-10 minutes to be fully operational. A local Kubernetes (kind) cluster will be configured and in it a sample application, AstroShop, will be deployed. To make your experience better, we are also installing and configuring tools like:</p> <p>k9s kubectl helm node jq python3 gh</p>"},{"location":"3-codespaces/#explore-codespace","title":"Explore Codespace","text":"<p>Your Codespace has now deployed the following resources:</p> <ul> <li> <p>A local Kubernetes (kind) cluster, with some pre-deployed apps that will be used later in the demo.</p> </li> <li> <p>CronJobs running in Kubernetes that generate some sample log data</p> </li> </ul> <p>After a couple of minutes, you'll see this screen in your Codespaces terminal. It contains the links to the UI of the application which we will be using for our hands-on training.</p> <p>Sample output: </p>"},{"location":"3-codespaces/#tips-tricks","title":"Tips &amp; Tricks","text":"<p>We want to boost your learning and try to make your experience as smooth as possible with Dynatrace trainings. Your Codespaces have a couple of convenience features added. </p>"},{"location":"3-codespaces/#show-the-greeting","title":"Show the greeting","text":"<p>In the terminal, there are functions loaded for your convenience. By creating a new terminal the greeting will be shown that includes the links to the exposed apps, the Github  pages, the Github Repository, the Dynatrace Tenant that is bound to this devcontainer (if applicable) and some of the tools installed.</p> <p>You can create a new terminal directly in VSCode, type <code>zsh</code> or call the function <code>printGreeting</code> and that will print the greeting with the most relevant information.</p>"},{"location":"3-codespaces/#navigating-in-your-local-kubernetes","title":"Navigating in your local Kubernetes","text":"<p>The client <code>kubectl</code> and <code>k9s</code> are configured so you can navigate in your local Kubernetes.  </p>"},{"location":"3-codespaces/#exposing-the-apps-to-the-public","title":"Exposing the apps to the public","text":"<p>The AstroShop app is being exposed in the devcontainer to your localhost. If you want to make the endpoints publicly accesible, just go to the ports section, right click on them and change the visibility to public.</p>"},{"location":"3-codespaces/#troubleshooting","title":"Troubleshooting","text":""},{"location":"3-codespaces/#astroshop","title":"AstroShop","text":"<p>If you encounter problems with the AstroShop app deployed in the <code>astroshop</code> namespace, you can easily recycle the pods.</p> <p>Recycle pods: <pre><code>kubectl delete pods --all -n astroshop\n</code></pre></p> <p>But before doing so, if you want to see what is happening we recommend the following: </p> <p>Verify all astroshop pods <pre><code>kubectl get pods -n astroshop\n</code></pre></p> <p>Check for events in the astroshop namespace <pre><code>kubectl get events -n astroshop\n</code></pre></p> <p>Check for system and cluster events  <pre><code>kubectl get events -n kube-system\nkubectl get events -n default\n</code></pre></p>"},{"location":"3-codespaces/#app-exposure","title":"App exposure","text":"<p>The Astroshop application is exposed via NodePort and it's mapping port 8080 to Cluster port 30100.</p> <p>Verify service: <pre><code>kubectl get svc astroshop-frontendproxy -n astroshop\n</code></pre></p>"},{"location":"3-codespaces/#cronjobs","title":"CronJobs","text":"<p>If you encounter problems with the CronJobs deployed in the <code>cronjobs</code> namespace, reapply the manifests.</p> <p>Deploy cronjobs: <pre><code>deployCronJobs\n</code></pre></p>"},{"location":"3-codespaces/#continue","title":"Continue","text":"<p>In the next section, we'll deploy Dynatrace on our Kubernetes cluster to start collecting logs (and more!).</p> <ul> <li>Continue to Deploy Dynatrace</li> </ul>"},{"location":"4-deploy-dynatrace/","title":"Deploy Dynatrace","text":"<p>Dynatrace provides integrated log management and analytics for your Kubernetes environments by either running the OneAgent Log Module or integrating with log collectors such as Fluent Bit, OpenTelemetry Collector, Logstash, or Fluentd.</p> <p>Dynatrace provides a flexible approach to Kubernetes observability where you can pick and choose the level of observability you need for your Kubernetes clusters. The Dynatrace Operator manages all the components needed to get the data into Dynatrace for you. This also applies to collecting logs from Kubernetes containers. Depending on the selected observability option, the Dynatrace Operator configures and manages the Log Module to work in conjunction with or without a OneAgent on the node.</p> <ul> <li>Learn More</li> </ul>"},{"location":"4-deploy-dynatrace/#kubernetes-platform-monitoring-application-observability","title":"Kubernetes Platform Monitoring + Application Observability","text":"<p>Kubernetes platform monitoring sets the foundation for understanding and troubleshooting your Kubernetes clusters. This setup does not include OneAgent or application-level monitoring by default, but it can be combined with other monitoring and injection approaches.</p> <p>Kubernetes Platform Monitoring: Capabilities</p> <ul> <li>Provides insights into the health and utilization of your Kubernetes clusters, including object relationships (topology)</li> <li>Uses the Kubernetes API and cAdvisor to get node- and container-level metrics and Kubernetes events</li> <li>Enables out-of-the-box alerting and anomaly detection for workloads, Pods, nodes, and clusters</li> </ul> <p>Application observability focuses on monitoring application-level metrics by injecting code modules into application Pods. This mode offers multiple injection strategies (automatic, runtime, and build-time) to collect application-specific metrics. For infrastructure-level insights, combine it with Kubernetes platform monitoring.</p> <p>Application Observability: Capabilities</p> <ul> <li>Dynatrace injects code modules into Pods using the Kubernetes admission controller.</li> <li>Get granular control over the instrumented Pods using namespaces and annotations.</li> <li>Route Pod metrics to different Dynatrace environments within the same Kubernetes cluster.</li> <li>Enable data enrichment for Kubernetes environments.</li> </ul>"},{"location":"4-deploy-dynatrace/#start-monitoring-kubernetes","title":"Start Monitoring Kubernetes","text":"<p>In your Dynatrace tenant, launch the <code>Kubernetes</code> app.  From the Overview tab, click on <code>Add cluster</code>.</p> <p></p> <p>1. Select distribution</p> <p>Choose <code>Other distributions</code> as your distribution, as we will be deploying Dynatrace on a generic Kind Kubernetes cluster.</p> <p>2. Select observability options</p> <p>Choose <code>Kubernetes platform monitoring + Application observability</code> as your observability option.  This will define your Dynakube spec/configuration.</p> <p>Toggle the <code>Log Management and Analytics</code> flag/setting to <code>Enabled</code>.  Expand the option and select <code>Fully managed with Dynatrace Log Module</code>.</p> <p>Check the box for <code>Restrict Log monitoring to certain resources</code>.  In the <code>Namespaces</code> field, type <code>astroshop</code>.  This will filter log ingestion on logs related to the <code>astroshop</code> Kubernetes namespace.</p> <p>Toggle the <code>Extensions</code> flag/setting to <code>Disabled</code>.  We will not be using this feature in this lab.</p> <p></p> <p>3. Configure cluster</p> <p>Give your Kubernetes cluster a name, enter <code>enablement-log-ingest-101</code>.</p> <p>4. Install Dynatrace Operator</p> <p>Generate a Dynatrace Operator Token.  Copy and save the value somewhere, in case you need it.  The value will automatically be added to the <code>dynakube.yaml</code> file.</p> <p>Generate a Data Ingest Token.  Copy and save the value somewhere, in case you need it.  The value will automatically be added to the <code>dynakube.yaml</code> file.</p> <p>Download the <code>dynakube.yaml</code> file.</p> <p>Copy the <code>helm install dynatrace-operator</code> command to your clipboard.  Use the command from your Dynatrace tenant, but it should look similar to this: <pre><code>helm install dynatrace-operator oci://public.ecr.aws/dynatrace/dynatrace-operator \\\n--create-namespace \\\n--namespace dynatrace \\\n--atomic\n</code></pre></p> <p></p>"},{"location":"4-deploy-dynatrace/#deploy-dynatrace-operator","title":"Deploy Dynatrace Operator","text":"<p>Navigate back to your GitHub Codespaces instance.  From the terminal, paste the <code>helm install dynatrace-operator</code> command and execute it.</p> <p></p> <p>Validate the new Dynatrace pods are running: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p>"},{"location":"4-deploy-dynatrace/#deploy-dynakube","title":"Deploy Dynakube","text":"<p>Locate the <code>dynakube.yaml</code> file that you downloaded from your tenant.  With the file (directory) open, navigate back to your GitHub Codespaces instance.  Click and hold to drag and drop the <code>dynakube.yaml</code> file into your Codespaces instance.</p> <p></p> <p>ActiveGate Container Resources</p> <p>Consider changing the ActiveGate's resources for better performance in this lab environment <pre><code>kind: DynaKube\nspec:\n  activeGate:\n    resources:\n      requests:\n        cpu: 100m\n        memory: 512Mi\n      limits:\n        cpu: 500m\n        memory: 768Mi\n</code></pre></p> <p>Deploy the Dynakube using <code>kubectl</code>. <pre><code>kubectl apply -f dynakube.yaml\n</code></pre></p> <p>Wait 3-5 minutes and validate that the Dynatrace pods are running. <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-oneagent-csi-driver-7b9kx 4/4 Running 0 3m5s dynatrace-operator-747d795b5c-hrmtl 1/1 Running 0 3m5s dynatrace-webhook-5b697d4b9d-6v95s 1/1 Running 0 3m5s dynatrace-webhook-5b697d4b9d-nvslc 1/1 Running 0 3m5s enablement-log-ingest-101-activegate-0 1/1 Running 0 90s enablement-log-ingest-101-logmonitoring-dxrsh 1/1 Running 0 89s"},{"location":"4-deploy-dynatrace/#dynakube-log-module-spec","title":"Dynakube Log Module Spec","text":"<p>Enabling Log Management and Analytics with the option <code>Fully managed with Dynatrace Log Module</code> will add the Log Module to the Dynakube spec.</p> <pre><code>---\napiVersion: dynatrace.com/v1beta3\nkind: DynaKube\nmetadata:\n  name: enablement-log-ingest-101\n  namespace: dynatrace\n  annotations:\n    feature.dynatrace.com/k8s-app-enabled: \"true\"\nspec:\n  apiUrl: https://&lt;tenant&gt;/api\n  metadataEnrichment:\n    enabled: true\n  oneAgent:\n    applicationMonitoring: {}\n  activeGate:\n    capabilities:\n      - routing\n      - kubernetes-monitoring\n    resources:\n      requests:\n        cpu: 100m\n        memory: 512Mi\n      limits:\n        cpu: 500m\n        memory: 768Mi\n  templates:\n    logMonitoring:\n      imageRef:\n        repository: public.ecr.aws/dynatrace/dynatrace-logmodule\n        tag: 1.309.66.20250401-150134\n  logMonitoring:\n    ingestRuleMatchers:\n      - attribute: k8s.namespace.name\n        values:\n          - astroshop\n</code></pre> <p>The Log Module runs as a container in a standalone pod (as part of a daemonset) on each node.  The <code>spec.templates.imageRef</code> defines the container image and tag to be used.</p> <pre><code>templates:\n    logMonitoring:\n      imageRef:\n        repository: public.ecr.aws/dynatrace/dynatrace-logmodule\n        tag: 1.309.66.20250401-150134\n</code></pre> <p>ImagePullBackOff Error</p> <p>In case you encounter an ImagePullBackOff error, check public.ecr.aws to make sure the container image with that tag exists.  If not, change the value to use an existing one. </p> <p>Enabling the option Restrict Log monitoring to certain resources option will add <code>spec.logMonitoring.ingestRuleMatchers</code> to the Dynakube definition.</p> <pre><code>logMonitoring:\n    ingestRuleMatchers:\n      - attribute: k8s.namespace.name\n        values:\n          - astroshop\n</code></pre> <p>Log Ingest Rule Configuration</p> <p>Log ingest for the Log Module is controlled by the Dynatrace tenant, not the local Dynakube configuration!  This configuration is a quality of life feature that is to be used during the initial deployment of Dynatrace on Kubernetes.</p> <p>By enabling the Log Module in your <code>dynakube.yaml</code> definition, this will enable the Dynakube to add a Log Ingest rule scoped at the Cluster-level within the Dynatrace tenant.</p> <p>As of Dynatrace Operator version <code>1.4.2</code> and Dynatrace version <code>1.311</code>, the Log Ingest rule is added upon deployment and creation of the Kubernetes Cluster setting, but any further changes within the Dynakube's configuration will not update the setting.  Manage Log Ingest rules within the Dynatrace tenant.  If you seek to automate this process at the Kubernetes Cluster-level, consider deploying Dynatrace Configuration As Code to manage the setting.</p> <ul> <li>Learn More</li> </ul>"},{"location":"4-deploy-dynatrace/#configure-log-ingest","title":"Configure Log Ingest","text":"<p>In your Dynatrace tenant, return to the <code>Kubernetes</code> app.  Click on the <code>Explorer</code> tab.  In your list of Clusters, click on <code>enablement-log-ingest-101</code>.</p> <p></p> <p>From the Cluster overview pop-out, in the top right corner, click on the <code>...</code> ellipsis icon, and then click on the drilldown for Log ingest rules.</p> <p></p> <p>This will open the <code>Kubernetes Classic</code> app and the connection settings for the Kubernetes Cluster.  In the Log Monitoring settings, the Log Ingest rules are shown.  You'll find the rule that was created by the Dynakube that matches the configuration in the <code>dynakube.yaml</code> spec.  It should be configured to only ingest logs from the <code>astroshop</code> namespace.</p> <p></p>"},{"location":"4-deploy-dynatrace/#refresh-application-pods","title":"Refresh Application Pods","text":"<p>Now that Dynatrace is deployed, let's refresh/recycle the application pods for <code>astroshop</code> to inject the OneAgent code modules.</p> <pre><code>kubectl delete pods -n astroshop --field-selector=\"status.phase=Running\"\n</code></pre>"},{"location":"4-deploy-dynatrace/#validate-log-ingest","title":"Validate Log Ingest","text":"<p>In your Dynatrace tenant, return to the <code>Kubernetes</code> app.  From the Cluster overview tab, click on <code>Namespaces</code> to open the list of Namespaces on the Cluster.</p> <p></p> <p>From the list of Namespaces, click on <code>astroshop</code>.  From the Namespace pop-out, click the <code>Logs</code> tab.  Verify in the chart that logs are being ingested for the <code>astroshop</code> namespace.  Click on <code>Run query</code> on the Show logs in current context option.</p> <p></p> <p>Validate log data after running the query.</p> <p></p>"},{"location":"4-deploy-dynatrace/#continue","title":"Continue","text":"<p>In the next section, we'll configure Log Monitoring in Dynatrace.</p> <ul> <li>Continue to configuring Dynatrace Log Monitoring</li> </ul>"},{"location":"5-configure-dynatrace/","title":"Configure Dynatrace","text":"<p>You can configure log ingestion rules in Dynatrace to control which logs should be collected from your Kubernetes environment. The rules leverage Kubernetes metadata and other common log entry attributes to determine which logs are to be ingested. The standard log processing features from OneAgent, including sensitive data masking, timestamp configuration, log boundary definition, and automatic enrichment of log records, are also available for Kubernetes logs.</p> <p>Dynatrace Automatic Log Ingest</p> <p>Dynatrace automatically discovers and analyzes new log files, including Kubernetes pod logs.  The out-of-the-box configuration will discover, parse, and ingest the logs from our <code>astroshop</code> application.  In order to exercise some of the advanced log monitoring configurations, we'll ingest sample CronJobs with logs that will need further configuration.</p>"},{"location":"5-configure-dynatrace/#ingest-cronjob-logs","title":"Ingest CronJob Logs","text":"<p>Dynatrace log ingest configuration allows you to remotely configure installed OneAgents to either include specific log sources for forwarding to Dynatrace or exclude them from upload. While log discovery refers to the automatic detection of log files so that no additional log source configuration effort is required on your environment, log ingestion involves the process of collecting logs and sending required log sources into Dynatrace.</p> <p>Log ingest configuration is based on rules that use matchers to target process groups, content, log levels, log paths, and other attributes described in this document. These rules determine which log files are ingested among those automatically detected by OneAgent or defined as custom log sources. </p> <p>Log ingest rules are ordered configurations processed from top to bottom. For higher configuration granularity, log ingest rules can be defined at four scopes: host, Kubernetes cluster, host group, and environment, with host scope rules having the highest priority.</p> <p></p>"},{"location":"5-configure-dynatrace/#log-ingest-rule","title":"Log Ingest Rule","text":"<p>In your Dynatrace tenant, open the (new) <code>Settings</code> app.  Select the <code>Collect and capture</code> submenu.  Click on the <code>Log monitoring</code> menu.  Click on <code>Log ingest rules</code> to open the setting in the <code>Settings Classic</code> app.</p> <p></p> <p>Here you will find the log ingest rules set at the environment-level.  Rules configured here will be inherited by every host group, Kubernetes cluster, and host in the environment.  However, these settings can be overridden at the granular entity-level.</p> <p>Click on <code>Hierarchy and overrides</code>.  Locate your Kubernetes cluster override for <code>enablement-log-ingest-101</code> and click on it.</p> <p></p> <p>Add a new rule that will capture logs from specific pods within the <code>cronjobs</code> namespace.  Click on <code>Add rule</code>.</p> <p></p> <p>Configure the rule with the following details:</p> <p>Rule Name: <pre><code>CronJob Logs\n</code></pre></p> <p>Rule Type: <pre><code>Include in storage\n</code></pre></p> <p>Conditions:</p> <p>Kubernetes namespace name is <pre><code>cronjobs\n</code></pre></p> <p>Kubernetes pod annotation is <pre><code>logs.dynatrace.io/ingest=true\n</code></pre></p> <p></p> <p>This rule will enable the Log Module to collect logs from pods that belong to the <code>cronjobs</code> namespace AND have the annotation <code>logs.dynatrace.io/ingest: true</code>.</p> <p>However, for this use case, configuring this rule alone will not start capturing these logs...</p>"},{"location":"5-configure-dynatrace/#log-module-feature-flag","title":"Log Module Feature Flag","text":"<p>Log Autodiscovery</p> <p>Logs will automatically be discovered by Dynatrace (OneAgent &amp; Log Module).  However, not all logs may be discovered by default.  A log file must meet all of the autodiscovery requirements to be autodiscovered!</p> <p>An enhancement to the log module for Kubernetes introduces a feature that enables Dynatrace to capture all container logs, even those that do not meet the autodiscovery requirements.  The logs from the CronJobs are written by a container echoing a message to stdout.  This prevents them from meeting the autodiscovery requirements, thus we will need to enable the feature flag to collect all container logs.</p> <ul> <li>Learn More</li> </ul> <p>In your Dynatrace tenant, return to the Kubernetes settings for your cluster where you configured the log ingest rule.  In the Log Monitoring section, click on <code>Log module feature flags</code>.  Enable the setting <code>Collect all container logs</code>.  Click on <code>Save changes</code>.</p> <p></p> <p>With this feature enabled and the log ingest rule configured, Dynatrace will now start to collect logs from the CronJob containers.</p>"},{"location":"5-configure-dynatrace/#query-cronjob-logs","title":"Query CronJob Logs","text":"<p>Validate that the logs are now being ingested into Dynatrace.  Open the <code>Logs</code> app.  Filter the logs on the <code>cronjobs</code> namespace and click <code>Run query</code>.</p> <pre><code>k8s.namespace.name = cronjobs\n</code></pre> <p></p>"},{"location":"5-configure-dynatrace/#configure-sensitive-data-masking","title":"Configure Sensitive Data Masking","text":"<p>Specific log messages may include user names, email addresses, URL parameters, and other information that you may not want to disclose. Log Monitoring features the ability to mask any information by modifying the configuration file on each OneAgent that handles information you consider to be sensitive.</p> <p>Masking is performed directly on OneAgent, ensuring that sensitive data are never ingested into the system.</p> <ul> <li>Learn More</li> </ul> <p>Within our CronJob logs, the <code>log-message-cronjob</code> writes out a message containing an email address.  While this email address isn't real, we can use it as an example of sensitive data masking.</p> <pre><code>$TIMESTAMP INFO Log message from cronjob.  email=example@dynatrace.io Ending job.\n</code></pre> <p>Built-In Sensitive Data Masking</p> <p>Dynatrace includes built-in sensitive data masking rules for email address, credit cards, URL queries, IBAN, and API-Tokens at the Environment-level.  If these settings are enabled, that may have already caused the email address in the CronJob log to be masked. </p>"},{"location":"5-configure-dynatrace/#sensitive-data-masking-rule","title":"Sensitive Data Masking Rule","text":"<p>In your Dynatrace tenant, return to the Kubernetes settings for your cluster where you configured the log ingest rule.  In the Log Monitoring section, click on <code>Sensitive data masking</code>.  Click on <code>Add rule</code>.</p> <p></p> <p>Configure the rule with the following details:</p> <p>Rule Name: <pre><code>CronJob Email Address\n</code></pre></p> <p>Search Expression: <pre><code>\\b[\\w\\-\\._]+?@[\\w\\-\\._]+?\\.\\w{2,10}?\\b\n</code></pre></p> <p>Masking Type: <pre><code>SHA-256\n</code></pre></p> <p>Conditions:</p> <p>Kubernetes namespace name is <pre><code>cronjobs\n</code></pre></p> <p></p> <p>The advantage of using the SHA-256 masking type is that every unique value (email address) will produce a unique hash value.  This may enable you to see if the log contains the same email address or different email addresses.  However, you will obviously not be able to decrypt what that email address is, keeping it secure!</p>"},{"location":"5-configure-dynatrace/#query-cronjob-logs_1","title":"Query CronJob Logs","text":"<p>The CronJob will execute every few minutes.  Allow some time for the job to run again and produce new log records.</p> <p>Return to the <code>Logs</code> app and filter on the logs that contain the email address.</p> <pre><code>k8s.namespace.name = cronjobs k8s.deployment.name = log-message-cronjob-* content = \"*email*\"\n</code></pre> <p></p> <p>The logs now contain the hashed value of the email address, <code>03cb5558c834be3796387a17763f315f22d3ab87cd1dc6d5d4817f8d27ec5913</code>.</p>"},{"location":"5-configure-dynatrace/#configure-timestampsplitting-patterns","title":"Configure Timestamp/Splitting Patterns","text":"<p>By default, log monitoring automatically detects only the most common and unambiguous subset of date formats supported. Each time a timestamp pattern is detected, the line will be treated as the beginning of the log entry. All following lines without a detected timestamp will be treated as a continuation and reported as a single multi-line log record.</p> <p>In the event that a multi-line log record contains another supported timestamp, it is likely that Dynatrace will treat that line as a new log record.  If this is not the desired result, a timestamp configuration rule can be created to change this behavior.</p> <ul> <li>Learn More</li> </ul> <p>In our CronJob logs, the <code>timestamp-cronjob</code> writes a multi-line log record that contains multiple timestamps.  Dynatrace treats the extra timestamps as new log lines.  We want this log record to be treated as a single record.</p> <p></p>"},{"location":"5-configure-dynatrace/#timestamp-configuration-rule","title":"Timestamp Configuration Rule","text":"<p>In your Dynatrace tenant, return to the Kubernetes settings for your cluster where you configured the log ingest rule.  In the Log Monitoring section, click on <code>Timestamp/Splitting patterns</code>.  Click on <code>Add rule</code>.</p> <p></p> <p>Configure the rule with the following details:</p> <p>Rule Name: <pre><code>Timestamp CronJob\n</code></pre></p> <p>Search Expression: <pre><code>%^%FT%T%z\n</code></pre></p> <p>Conditions:</p> <p>Kubernetes namespace name is <pre><code>cronjobs\n</code></pre></p> <p></p> <p>Here is a breakdown of our pattern:</p> Entry Description %^ Matches timestamp patterns only at the start of a log line %F Shortcut for %Y-%m-%d, 2025-11-30 for example T The literal character 'T' as found in the ISO 8601  format %T Shortcut for %H-%M-%S, 12:30:01 for example %z Timezone indicator as found in the ISO 8601 format <p>This rule will cause Dynatrace to stop splitting log records on new lines with timestamps, since they don't appear at the beginning of the log record.</p>"},{"location":"5-configure-dynatrace/#query-logs","title":"Query Logs","text":"<p>The CronJob will execute every few minutes.  Allow some time for the job to run again and produce new log records.</p> <p>Return to the <code>Logs</code> app and filter on the logs that contain the multiple timestamps.</p> <pre><code>k8s.namespace.name = cronjobs content != \"*injection-startup*\" k8s.workload.name=\"timestamp-cronjob\"\n</code></pre> <p></p> <p>Each log message is now treated as a single, multi-line, log record containing the entire message.</p>"},{"location":"5-configure-dynatrace/#ingest-dynatrace-logs","title":"Ingest Dynatrace Logs","text":"<p>In some cases, you may want to collect container logs from the Dynatrace components running in the <code>dynatrace</code> namespace.  By default, collection of these logs is disabled, even if you have a log ingest rule configured to do so.  Logs collected from the Dynatrace components are treated like any other log that you ingest - it consumes licensing, storage, etc.</p> <p>In your Dynatrace tenant, return to the Kubernetes settings for your cluster where you configured the log ingest rule.  In the Log Monitoring section, click on <code>Advanced log settings</code>.  Enable the setting <code>Allow OneAgent to monitor Dynatrace logs</code>.  Click on <code>Save changes</code>.</p> <p></p> <p>This allows the Log Module to discover the Dynatrace component logs.  However, we need to add an ingest rule to ship them to Dynatrace.</p> <p>In the Log Monitoring section, click on <code>Log ingest rules</code>.  Modify your existing rule called <code>enablement-log-ingest-101</code> and add the <code>dynatrace</code> namespace to the matcher.  Save your changes.</p> <p></p>"},{"location":"5-configure-dynatrace/#query-logs_1","title":"Query Logs","text":"<p>Return to the <code>Logs</code> app and filter on the logs from the <code>dynatrace</code> namespace.</p> <pre><code>k8s.namespace.name = dynatrace\n</code></pre> <p></p> <p>These logs can help with troubleshooting any observability issues on the Kubernetes cluster.  However, it is the Log Module that is collecting these logs, so if the Log Module is not working - the logs won't be shipped to Dynatrace!</p>"},{"location":"5-configure-dynatrace/#configure-openpipeline","title":"Configure OpenPipeline","text":"<p>OpenPipeline is the Dynatrace data handling solution to seamlessly ingest and process data from different sources, at any scale, and in any format in the Dynatrace Platform.</p> <p>Dynatrace OpenPipeline can reshape incoming data for better understanding, processing, and analysis. OpenPipeline processing is based on rules that you create and offers a flexible way of extracting value from raw records.</p> <p></p> <ul> <li>Learn More</li> </ul> <p>The logs written by the <code>paymentservice</code> within the <code>astroshop</code> application are missing some important context information, making them a great candidate for ingest processing with OpenPipeline.</p> <p>Sample log snippet: <pre><code>{\"level\":30,\"time\":1744936198898,\"pid\":1,\"hostname\":\"astroshop-paymentservice-7dbc46ff58-4msdx\",\"dt.entity.host\":\"HOST-815866271C8841E1\",\"dt.entity.kubernetes_cluster\":\"KUBERNETES_CLUSTER-FD9401B313C80ED9\",\"dt.entity.process_group\":\"PROCESS_GROUP-F8DE358ACD7BA713\",\"dt.entity.process_group_instance\":\"PROCESS_GROUP_INSTANCE-BE5376E9380A3175\",\"dt.kubernetes.cluster.id\":\"dfc521db-c6be-471f-9e20-24f62e45bb69\",\"dt.kubernetes.workload.kind\":\"deployment\",\"dt.kubernetes.workload.name\":\"astroshop-paymentservice\",\"k8s.cluster.name\":\"enablement-log-ingest-101\",\"k8s.cluster.uid\":\"dfc521db-c6be-471f-9e20-24f62e45bb69\",\"k8s.container.name\":\"paymentservice\",\"k8s.namespace.name\":\"astroshop\",\"k8s.node.name\":\"kind-control-plane\",\"k8s.pod.name\":\"astroshop-paymentservice-7dbc46ff58-4msdx\",\"k8s.pod.uid\":\"8992e584-dc50-48f9-b243-1facae6d1ba5\",\"k8s.workload.kind\":\"deployment\",\"k8s.workload.name\":\"astroshop-paymentservice\",\"process.technology\":\"nodejs\",\"dt.trace_id\":\"6977c25e6f6a6b4fcc0117087990d95b\",\"dt.span_id\":\"54f824774f71e56c\",\"dt.trace_sampled\":\"true\",\"transactionId\":\"f762bcae-fdb7-4f55-9576-a99273683d12\",\"cardType\":\"visa\",\"lastFourDigits\":\"1278\",\"amount\":{\"units\":{\"low\":1173,\"high\":0,\"unsigned\":false},\"nanos\":999999999,\"currencyCode\":\"USD\"},\"msg\":\"Transaction complete.\"}\n</code></pre></p>"},{"location":"5-configure-dynatrace/#configure-custom-logs-pipeline","title":"Configure Custom Logs Pipeline","text":"<p>In your Dynatrace tenant, open the <code>OpenPipeline</code> app.  Select <code>Logs</code> and click on the <code>Pipelines</code> tab.  Add a new Pipeline by clicking on <code>+ Pipeline</code>.</p> <p></p> <p>Give the new pipeline a name, <code>AstroShop PaymentService</code>.  Click on the <code>Processing</code> tab to create processing rules.</p> <p></p> <p>Save Your Configuration</p> <p>It is highly recommended to save your progress often by clicking the <code>Save</code> button and then re-opening your pipeline configuration to avoid losing your changes!</p>"},{"location":"5-configure-dynatrace/#processing-rules","title":"Processing Rules","text":"<p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>NodeJS\n</code></pre></p> <p>Type: <pre><code>Technology Bundle: NodeJS\n</code></pre></p> <p>Matching condition: <pre><code>Pre-defined: matchesValue(process.technology, \"Node.js\") or matchesValue(process.technology, \"nodejs\")\n</code></pre></p> <p>This processor rule will apply built-in pattern detection for known NodeJS technology log frameworks to parse and enrich the log content.  In our example, this will be important to extract the numeric <code>loglevel</code> into a recognizable string value; such as \"INFO\" or \"WARN\".</p> <p></p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>Parse Content\n</code></pre></p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(log.raw_level) and loglevel != \"NONE\" and isNotNull(message)\n</code></pre></p> <p>DQL processor definition: <pre><code>parse content, \"JSON:json_content\"\n| fieldsAdd content = json_content\n| fieldsFlatten content, depth: 3\n</code></pre></p> <p>This processor rule will parse the JSON structured <code>content</code> field and flatten the object into new fields.  This will make accessing the details in the content field much easier.</p> <p></p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>Message to Content\n</code></pre></p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(log.raw_level) and loglevel != \"NONE\" and isNotNull(message) and isNotNull(json_content)\n</code></pre></p> <p>DQL processor definition: <pre><code>fieldsRemove content\n| fieldsAdd content = message\n</code></pre></p> <p>This processor rule will replace the <code>content</code> field value with the <code>message</code> field value.  The JSON object in the content field is excessive and contains a ton of information that isn't relative to the field itself.  The same information already exists on the log, outside of the content field.</p> <p></p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>Transaction Fields\n</code></pre></p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(content,\"Transaction complete.\") and isNotNull(content.amount.units.low) and isNotNull(content.transactionId)\n</code></pre></p> <p>DQL processor definition: <pre><code>fieldsAdd payment.amount = content.amount.units.low\n| fieldsAdd payment.transactionid = content.transactionId\n| fieldsAdd payment.currencycode = content.amount.currencyCode\n| fieldsAdd payment.cardtype = content.cardType\n| fieldsAdd payment.lastfourdigits = content.lastFourDigits\n</code></pre></p> <p>This processor rule will simplify the field names for extracting business information.  When the service successfully processes a payment transaction, a log record is written with the payment information.  We want this information extracted for business observability use cases.</p> <p></p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>Mask Sensitive Data\n</code></pre></p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(content.request.creditCard.creditCardNumber)\n</code></pre></p> <p>DQL processor definition: <pre><code>fieldsAdd maskedcreditcardnumber = hashMd5(content.request.creditCard.creditCardNumber)\n| fieldsRemove content.request.creditCard.creditCardNumber\n</code></pre></p> <p>This processor rule will create a new field containing the MD5 hash value of the credit card number.  This is an example of masking sensitive data at ingest using OpenPipeline.</p> <p></p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>Cleanup Fields\n</code></pre></p> <p>Type: <pre><code>DQL\n</code></pre></p> <p>Matching condition: <pre><code>isNotNull(log.raw_level) and loglevel != \"NONE\" and isNotNull(message) and isNotNull(json_content) and isNotNull(content)\n</code></pre></p> <p>DQL processor definition: <pre><code>fieldsRemove \"content.dt.*\", \"content.k8s.*\", \"content.time\",\"content.level\", \"content.process.technology\", \"content.hostname\", \"content.pid\"\n| fieldsRemove \"content.request.creditCard.*\"\n| fieldsRemove json_content\n</code></pre></p> <p>This processor rule will cleanup (remove) fields that are no longer needed.  Parsing the <code>content</code> field resulted in many unwanted fields prefixed with <code>content.*</code>.  Additionally, the payment request log records contain sensitive credit card data (spoofed of course).  This processor will remove those fields with sensitive data.  This is an example of masking or deleting sensitive data at ingest using OpenPipeline.</p> <p></p>"},{"location":"5-configure-dynatrace/#data-extraction","title":"Data Extraction","text":"<p>Click on the <code>Data extraction</code> tab to create data extraction rules.</p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>PaymentService Transaction\n</code></pre></p> <p>Type: <pre><code>Business Event\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(content,\"Transaction complete.\") and isNotNull(payment.amount) and isNotNull(payment.transactionid)\n</code></pre></p> <p>Event type: <pre><code>astroshop.paymentservice.transaction.complete\n</code></pre></p> <p>Event provider: <pre><code>astroshop\n</code></pre></p> <p>Field extraction: <pre><code>Extract all fields\n</code></pre></p> <p>This data extraction rule will generate a business event (bizevent) anytime the payment service successfully completes a payment transaction, as identified by the matching log message.  This new business event can then be used for business analytics and business observability use cases.</p> <p></p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>PaymentService Fail Feature Flag Enabled\n</code></pre></p> <p>Type: <pre><code>Davis Event\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(status,\"WARN\") and matchesValue(message,\"PaymentService Fail Feature Flag Enabled\") and isNotNull(dt.entity.cloud_application)\n</code></pre></p> <p>Event name: <pre><code>PaymentService Fail Feature Flag Enabled\n</code></pre></p> <p>Event description: <pre><code>Generate an error when calling the charge method.\n</code></pre></p> <p>Event properties:</p> Field Value event.type ERROR_EVENT event.name PaymentService Fail Feature Flag Enabled event.description Generate an error when calling the charge method. dt.davis.is_rootcause_relevant true dt.davis.is_merging_allowed true dt.source_entity {dt.entity.cloud_application} <p>This data extraction rule will generate a Davis event (alert) anytime the payment service fails to process a payment transaction due to the problem pattern (feature flag) being enabled.  This is an example of using OpenPipeline to alert on log data at ingest.</p> <p></p>"},{"location":"5-configure-dynatrace/#metric-extraction","title":"Metric Extraction","text":"<p>Click on the <code>Metric extraction</code> tab to create metric extraction rules.</p> <p>Add a new processor rule by clicking on <code>+ Processor</code>.  Configure the processor rule with the following:</p> <p>Name: <pre><code>PaymentService Failure\n</code></pre></p> <p>Type: <pre><code>Counter metric\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(status,\"WARN\") and matchesValue(message,\"PaymentService Fail Feature Flag Enabled\") and isNotNull(dt.entity.cloud_application)\n</code></pre></p> <p>Metric key: <pre><code>log.astroshop.paymentservice.failure\n</code></pre></p> <p>Dimensions:</p> Pre-defined fields dt.entity.cloud_application dt.entity.process_group k8s.namespace.name k8s.workload.name <p>This metric extraction rule will create a counter metric to count the number of payment service failures caused by the feature flag being enabled.  Using DQL, one could count the number of log messages, summarize or make a timeseries of that data on the fly.  However, in order to optimize query costs and reduce raw log query volume, converting specific log data to metrics is a best practice for a use case such as this.</p> <p></p> <p>Permission and Storage Processors</p> <p>OpenPipeline can also be used to set the security context of the log data to manage permissions to specific logs.  Additionally, you can use storage processor rules to store specific log data in a bucket that is configured for a desired retention time.  For this lab, we will not create permission or storage processor rules.</p> <p>Click on <code>Save</code> to finish and save your new pipeline.</p>"},{"location":"5-configure-dynatrace/#dynamic-route","title":"Dynamic Route","text":"<p>A pipeline will not have any effect unless logs are configured to be routed to the pipeline. With dynamic routing, data is routed based on a matching condition. The matching condition is a DQL query that defines the data set you want to route.</p> <p>Click on <code>Dynamic Routing</code> to configure a route to the target pipeline. Click on <code>+ Dynamic Route</code> to add a new route.</p> <p></p> <p>Configure the <code>Dynamic Route</code> to use the <code>AstroShop PaymentService</code> pipeline.</p> <p>Name: <pre><code>AstroShop PaymentService\n</code></pre></p> <p>Matching condition: <pre><code>matchesValue(k8s.namespace.name,\"astroshop\") and matchesValue(k8s.container.name,\"paymentservice\")\n</code></pre></p> <p>Pipeline: <pre><code>AstroShop PaymentService\n</code></pre></p> <p>Click <code>Save</code> to add your new route.</p> <p></p> <p>Validate that the route is enabled in the <code>Status</code> column. Click on <code>Save</code> to save the dynamic route table configuration.</p> <p></p> <p>Allow the <code>paymentservice</code> from <code>astroshop</code> to generate new log data that will be routed through the new pipeline (3-5 minutes).</p>"},{"location":"5-configure-dynatrace/#query-logs_2","title":"Query Logs","text":"<p>Return to the <code>Logs</code> app and filter on the logs generated by the payment service.</p> <pre><code>k8s.namespace.name = \"astroshop\" k8s.container.name = \"paymentservice\" \n</code></pre> <p></p>"},{"location":"5-configure-dynatrace/#continue","title":"Continue","text":"<p>In the next section, we'll query, view, and analyze logs ingested into Dynatrace.</p> <ul> <li>Continue to analyzing logs in Dynatrace</li> </ul>"},{"location":"6-analyze-logs/","title":"Analyze Logs","text":""},{"location":"6-analyze-logs/#enable-astroshop-problem-patterns","title":"Enable Astroshop Problem Patterns","text":"<p>The <code>astroshop</code> demo provides several feature flags that you can use to simulate different scenarios. These flags are managed by flagd, a simple feature flag service that supports OpenFeature.</p> <p>Flag values can be changed through the user interface provided at http://localhost:8080/feature when running the demo. Changing the values through this user interface will be reflected in the flagd service.</p> <p>Navigate to the feature flag user interface by adding <code>/feature</code> to the end of your Codespaces instance URL.</p> <p>For example: <code>https://super-duper-capybara-4wvj9xx7wpjh76qr-30100.app.github.dev/feature</code></p> <p>Locate the flag paymentServiceFailure.  Click the drop down box and change it from <code>off</code> to <code>on</code>.  Click <code>save</code> at the top of the page.  The feature flag should start working within a minute.</p> <p></p>"},{"location":"6-analyze-logs/#analyze-logs-in-context","title":"Analyze Logs in Context","text":"<p>Modern applications run in distributed environments. They generate observability data like metrics, logs and traces. Having all data in one place is often not enough because manual correlation can be required. Understanding the behavior and performance of distributed applications is important for effective troubleshooting. Dynatrace automatically connects and puts data in context for a smooth troubleshooting and analytics experience. This automated approach not only streamlines troubleshooting but also enhances the overall analytics experience, enabling teams to optimize application performance with ease.</p> <ul> <li>Learn More</li> </ul> <p>Shortly after enabling the feature flag for <code>paymentServiceFailure</code>, the Payment Service should start to fail every single payment transaction.  Let's observe, using logs in context, the impact on application reliability of this change.</p>"},{"location":"6-analyze-logs/#problems-app","title":"Problems App","text":"<p>Start by opening the (new) <code>Problems</code> app.  By now, Dynatrace - powered by the Davis AI engine, should have detected a problem with the Payment Service through the ingested log records.  Locate the problem and open it to view the details.</p> <p>Notice the logs in context callout on the top right of the frame.  Dynatrace automatically searches for logs related to the entities that are root cause relevant.  Click on <code>Run query</code> to query the relevant logs.</p> <p></p> <p>Relevant logs are queried based on the impacted entity using a pre-built DQL query.  Without any manual intervention or context switching, Dynatrace surfaces the root cause relevant logs.  The logs have the error message and even the exception stacktrace information the developers would need to debug the issue.</p> <p></p> <p>Davis CoPilot Problem Explanation</p> <p>Davis CoPilot provides clear summaries of problems, their root causes, and the suggested remediation steps. Davis CoPilot explains individual issues in clear language from the problem details page and can perform a comparative analysis when multiple problems are selected from the list view. This helps you identify common root causes and propose corrective steps without relying on a team of experts and waiting for hours for critical insights.</p> <p>If your Dynatrace tenant has Davis CoPilot capabilities enabled (optional, not part of this lab) then you should see a button that says <code>Explain</code>.  Click it to open a prompt that will automatically ask Davis CoPilot to explain the problem in natural language and suggest remediation steps! </p> <p></p>"},{"location":"6-analyze-logs/#kubernetes-app","title":"Kubernetes App","text":"<p>Next, let's approach this issue in the context of our Kubernetes environment.  Open the (new) <code>Kubernetes</code> app and click on the <code>Overview</code> tab at the top.</p> <p>Notice the Workloads.  At least 1 workload should be identified as unhealthy, depending on what else you have monitored in your Dynatrace environment.  Click on <code>Workloads</code> to view them.</p> <p></p> <p>All of the workloads are displayed. Click on the unhealthy workloads indicator to filter the view on just those that Dynatrace has identified as unhealthy.</p> <p></p> <p>Now that only the unhealthy workloads are displayed, locate and click on the <code>astroshop-paymentservice</code> workload.  At the top, the Davis AI health indicators show how/why the workload is considered unhealthy.  There is a problem detected that's impacting this workload entity.  At the same time, we can immediate identify that there aren't issues with the workload related to Kubernetes workload conditions, CPU or memory resource usage, running/ready pods, or with the container health.</p> <p>Let's explore the logs for this workload in the context of the Kubernetes entity.  Click on the <code>Logs</code> tab.</p> <p></p> <p>Dynatrace highlights that there are logs relevant to the root cause of the problem for this workload.  Additionally, the Davis AI recommends a query that can be executed to view these relevant log records.  Click on <code>Run query</code>.</p> <p></p> <p>Relevant logs are queried based on the impacted Kubernetes entity using a pre-built DQL query.  Without any manual intervention or context switching, Dynatrace surfaces the root cause relevant logs.  The logs have the error message and even the exception stacktrace information the developers would need to debug the issue.</p> <p></p> <p>Connecting log data to traces</p> <p>Dynatrace can enrich your ingested log data with additional information that helps Dynatrace to recognize, correlate, and evaluate the data.  Log enrichment enables you to seamlessly switch context and analyze individual spans, transactions, or entire workloads + empower development teams by making it easier and faster for them to detect and pinpoint problems.</p> <p>From here, we can view the correlated distributed trace for the transaction that wrote this log record.  In the log record, specifically one of the log records where <code>content = PaymentService Fail Feature Flag Enabled</code>, locate the <code>trace_id</code> field.  Click on it and choose <code>Open field with</code>.  From there, choose the <code>Distributed Tracing</code> app to view the trace.</p> <p></p>"},{"location":"6-analyze-logs/#distributed-tracing-and-services-apps","title":"Distributed Tracing and Services Apps","text":"<p>A distributed trace is a collection of spans representing a request's journey through a distributed system.</p> <p>The request is the call initiated by a user or system to perform a specific task. It interacts with various services and components within the distributed system. Spans are individual operations representing each request interaction with the distributed system.</p> <p>The <code>Distributed Tracing</code> app will open already filtered to view the trace from the log record based on the <code>trace_id</code>.  From the trace waterfall view, expand and locate the <code>charge</code> span from the <code>PaymentService</code>.  The span includes response time information, but it also includes a lot of valuable span attributes and attached span events.  In the <code>Span Events</code> section, locate the <code>Exception</code> details.  Here you can see where the transaction encountered the exception that caused the transaction to fail.</p> <p></p> <p>Dynatrace automatically aggregates trace data into a <code>service</code> entity.  Traces, and ultimately spans, are analyzed out-of-the-box to measure the health of your endpoints and transactions across web, messaging, database, and other technologies.  Dynatrace provides aggregated metrics by default for things like response time, throughput, failure rate, and resource usage.  This allows you to easily find problems with your endpoints, identify hotspots, and analyze relevant trace data in real-time.</p> <p>This span belongs to the <code>PaymentService</code>.  Open the service in the (new) <code>Services</code> app by click on <code>oteldemo.PaymentService</code>.</p> <p></p> <p>The <code>Services</code> app opens with the <code>PaymentService</code> selected.  Here you can view the failure rate, response time, and throughput metrics for this service.  From here, you can drill down into more distributed traces for this service, find any correlated log records, view infrastructure health including Kubernetes entity details, and understand the topology and dependencies of this service.</p> <p></p> <p>Having logs, together and in context with metrics and traces, is essential to having a unified observability strategy.  Logs, metrics, and traces together is nice to have, but correlating them together and in context with application and infrastructure topology greatly speeds up troubleshooting.  Logs in context allow you to make better real-time business decisions by understanding business outcomes correlated with underlying system health.</p>"},{"location":"6-analyze-logs/#continue","title":"Continue","text":"<p>Now that the lab has been completed, in the next section we will clean up the codespaces instance.</p> <ul> <li>Continue to cleanup</li> </ul>"},{"location":"cleanup/","title":"7. Cleanup","text":"<p>Deleting the codespace from inside the container</p> <p>We like to make your life easier, for convenience there is a function loaded in the shell of the Codespace for deleting the codespace, just type <code>deleteCodespace</code>. This will trigger the deletion of the codespace.</p> <p>Another way to do this is by going to https://github.com/codespaces and delete the codespace.</p> <p>You may also want to deactivate or delete the API token needed for this lab.</p>"},{"location":"snippets/admonitions/","title":"Admonitions","text":"<p>Warning</p> <p>This is a Warning </p> <p>Note</p> <p>This is a Note </p> <p>Important</p> <p>This is important </p> <p>Tipp</p> <p>This is a tipp </p>"},{"location":"snippets/disclaimer/","title":"Disclaimer","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"snippets/requirements/","title":"Requirements","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"snippets/view-code/","title":"View code","text":"<p>View the Code</p> <p>The code for this repository is hosted on GitHub. Click the \"View Code on GitHub\" link above.</p>"}]}